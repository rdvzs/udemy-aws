
# Chapter 12: Advanced Amazon S3 & Athena

This chapter explores advanced features of Amazon S3 for data management and performance optimization, and introduces Amazon Athena for serverless data analysis.

## 12.1: S3 Lifecycle Rules
- **Purpose:** To automate the transition of objects between different storage classes to optimize costs.
- **Actions:**
  - **Transition Actions:** Move objects to a different storage class after a certain number of days (e.g., move to `S3 Standard-IA` after 30 days, then to `S3 Glacier Flexible Retrieval` after 180 days).
  - **Expiration Actions:** Configure objects to expire (be deleted) after a specified period. This can be applied to current versions, previous versions, or incomplete multipart uploads.
- **Scope:** Rules can be applied to an entire bucket, or scoped to a specific prefix (folder) or object tags.
- **S3 Analytics:** A feature that can be enabled to analyze access patterns on your objects and recommend optimal lifecycle rules for transitioning data between `S3 Standard` and `S3 Standard-IA`.

## 12.2: S3 Event Notifications
- **Functionality:** S3 can trigger events when certain actions occur in your bucket (e.g., an object is created, deleted, or restored).
- **Destinations:** These event notifications can be sent to:
  - **SNS (Simple Notification Service):** To fan out notifications to multiple subscribers.
  - **SQS (Simple Queue Service):** To queue events for reliable, asynchronous processing.
  - **Lambda Function:** To directly invoke a function to process the event.
- **EventBridge Integration:** S3 events are also automatically sent to Amazon EventBridge, which allows for more advanced filtering and routing to over 18 different AWS services.
- **Use Case:** A classic example is triggering a Lambda function to automatically create a thumbnail whenever a new image is uploaded to an S3 bucket.

## 12.3: S3 Performance Optimization
- **Baseline Performance:** S3 automatically scales to provide very high request rates. By default, it supports **3,500 PUT/POST/DELETE** and **5,500 GET/HEAD** requests per second, *per prefix*.
- **Prefixes:** A prefix is the "folder path" part of an object's key (e.g., in `my-folder/my-file.txt`, the prefix is `my-folder/`). Spreading your objects across multiple prefixes allows you to scale your request rate horizontally.
- **Multi-Part Upload:** For large objects (>100 MB is recommended, >5 GB is required), this feature splits the object into smaller parts that can be uploaded in parallel. This improves upload speed, increases resilience to network errors (only failed parts need to be retried), and allows you to pause and resume uploads.
- **S3 Transfer Acceleration:** Speeds up long-distance uploads and downloads by routing traffic through a nearby AWS Edge Location over the public internet, and then over the optimized AWS global network to the S3 bucket.
- **S3 Byte-Range Fetches:** Allows you to retrieve only a specific portion of an object, rather than the whole file. This can be used to parallelize downloads or to retrieve just the header of a file for inspection.

## 12.4: S3 Batch Operations
- **Functionality:** A managed service to perform bulk operations on a large number of S3 objects with a single request.
- **Operations:**
  - Copy objects between buckets.
  - Modify object metadata, tags, or ACLs.
  - Encrypt unencrypted objects.
  - Restore objects from S3 Glacier.
  - Invoke a Lambda function for custom actions on each object.
- **Process:** You provide a manifest (a list of objects, often generated by S3 Inventory) and specify the operation. S3 Batch Operations then manages the job, including retries, tracking, and completion reports.

## 12.5: S3 Inventory
- **Functionality:** Provides a flat file output (CSV, ORC, or Parquet) listing your objects and their corresponding metadata on a daily or weekly basis.
- **Use Case:** A scalable alternative to the `ListObjects` API call for auditing and reporting on the replication and encryption status of your objects. The output can be analyzed with tools like Amazon Athena.

## 12.6: S3 Glacier
- **S3 Glacier:** A set of low-cost, long-term archival storage classes. Data is stored in **Vaults**, and each item is an **Archive**.
- **Retrieval Tiers:**
  - **Expedited:** 1-5 minutes (most expensive).
  - **Standard:** 3-5 hours.
  - **Bulk:** 5-12 hours (cheapest).
- **Glacier Vault Lock:** A compliance feature that allows you to enforce policies on a vault, such as a Write-Once-Read-Many (WORM) policy. Once a Vault Lock policy is locked, it is **immutable** and cannot be changed or deleted.

## 12.7: Amazon Athena
- **What is it?** A serverless, interactive query service that makes it easy to analyze data directly in Amazon S3 using standard SQL.
- **How it Works:** You define a table schema over your data in S3, and then you can run SQL queries against it. Athena is serverless, so there is no infrastructure to manage.
- **Pricing:** You pay per query, based on the amount of data scanned ($5 per TB).
- **Performance Optimization:**
  - Use **columnar data formats** like Apache Parquet or ORC. This allows Athena to only read the columns needed for a query, drastically reducing the amount of data scanned.
  - **Compress** your data (e.g., with GZIP or Snappy).
  - **Partition** your data in S3 based on common query filters (e.g., `/year=2023/month=09/day=25/`). This allows Athena to skip reading data in irrelevant partitions.
- **Federated Query:** Athena can use Data Source Connectors (which are Lambda functions) to run queries on data stored outside of S3, including relational databases (RDS), NoSQL databases (DynamoDB), and even on-premises data sources.
